# üîí Guide : H√©berger le YouTube Summarizer sur votre VPS (100% Local)

Ce guide vous explique comment h√©berger l'application sur votre propre VPS pour que **toutes vos donn√©es restent sous votre contr√¥le** et ne soient jamais envoy√©es aux √âtats-Unis ou √† des services tiers.

---

## üìã Table des mati√®res

1. [Pr√©requis VPS](#pr√©requis-vps)
2. [Option 1 : Ollama (Recommand√©)](#option-1--ollama-recommand√©)
3. [Option 2 : vLLM (Pour GPU puissants)](#option-2--vllm-pour-gpu-puissants)
4. [Option 3 : Text Generation Inference](#option-3--text-generation-inference)
5. [Configuration de l'application](#configuration-de-lapplication)
6. [Acc√®s distant s√©curis√©](#acc√®s-distant-s√©curis√©)
7. [Comparaison des solutions](#comparaison-des-solutions)

---

## üñ•Ô∏è Pr√©requis VPS

### Configuration minimale recommand√©e :

| Composant | Minimum | Recommand√© | Id√©al |
|-----------|---------|------------|-------|
| **RAM** | 16 GB | 32 GB | 64 GB |
| **CPU** | 4 c≈ìurs | 8 c≈ìurs | 16+ c≈ìurs |
| **GPU** | Optionnel | NVIDIA 8GB VRAM | NVIDIA 24GB VRAM |
| **Stockage** | 50 GB | 100 GB | 200 GB |
| **Bande passante** | 100 Mbps | 1 Gbps | 1 Gbps |

### OS recommand√© :
- Ubuntu 22.04 LTS ou 24.04 LTS
- Debian 12
- Rocky Linux 9

---

## üöÄ Option 1 : Ollama (Recommand√©)

### ‚úÖ Avantages :
- Installation ultra-simple (1 commande)
- Gestion automatique des mod√®les
- Faible consommation de ressources
- Optimis√© pour CPU et GPU
- Communaut√© active

### üì¶ Installation sur votre VPS

```bash
# 1. Se connecter √† votre VPS
ssh votre-utilisateur@votre-vps-ip

# 2. Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 3. T√©l√©charger un mod√®le
# Pour 16 GB RAM (version quantifi√©e, plus l√©g√®re)
ollama pull llama3.1:8b-instruct-q4_0

# Pour 32+ GB RAM (version compl√®te)
ollama pull llama3.1:8b

# Alternatives performantes
ollama pull mistral:7b          # Excellent pour le fran√ßais
ollama pull gemma2:9b           # Tr√®s performant
ollama pull qwen2.5:7b          # Bon multilingue

# 4. Lancer le serveur Ollama
ollama serve

# Pour lancer en arri√®re-plan avec systemd (recommand√©)
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama
```

### üîß Configuration r√©seau

Par d√©faut, Ollama √©coute sur `localhost:11434`. Pour y acc√©der depuis d'autres machines :

```bash
# Ouvrir le port dans le firewall (UFW sur Ubuntu)
sudo ufw allow 11434/tcp

# Ou configurer Ollama pour √©couter sur toutes les interfaces
# √âditer le fichier de service
sudo systemctl edit ollama

# Ajouter ces lignes :
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Red√©marrer
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### üì± Utiliser l'application locale

```bash
# Sur votre VPS, installer les d√©pendances Python
pip install langchain langchain-community

# Lancer l'application locale
streamlit run app_local.py --server.port 8501 --server.address 0.0.0.0

# Ouvrir le port Streamlit
sudo ufw allow 8501/tcp

# Acc√©der depuis votre navigateur
# http://votre-vps-ip:8501
```

### üß™ Tester la connexion

```bash
# Test local
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Hello, world!",
  "stream": false
}'

# Test distant (depuis votre PC)
curl http://votre-vps-ip:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Bonjour!",
  "stream": false
}'
```

---

## üéØ Option 2 : vLLM (Pour GPU puissants)

### ‚úÖ Avantages :
- Performance maximale avec GPU
- Support de nombreux mod√®les
- Optimisations avanc√©es
- API compatible OpenAI

### ‚ö†Ô∏è Pr√©requis :
- GPU NVIDIA avec 16+ GB VRAM
- CUDA install√©
- Plus complexe √† configurer

### üì¶ Installation

```bash
# 1. Installer CUDA (si pas d√©j√† fait)
# Suivre : https://developer.nvidia.com/cuda-downloads

# 2. Installer vLLM
pip install vllm

# 3. T√©l√©charger un mod√®le depuis Hugging Face
# Exemple : Meta-Llama-3.1-8B-Instruct
huggingface-cli login  # Entrer votre token HF

# 4. Lancer le serveur vLLM
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Meta-Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000
```

### üîß Adapter l'application pour vLLM

Remplacer dans le code :
```python
from langchain_community.llms import VLLMOpenAI

llm = VLLMOpenAI(
    openai_api_key="EMPTY",
    openai_api_base="http://votre-vps-ip:8000/v1",
    model_name="meta-llama/Meta-Llama-3.1-8B-Instruct"
)
```

---

## üèóÔ∏è Option 3 : Text Generation Inference (TGI)

### ‚úÖ Avantages :
- D√©velopp√© par Hugging Face
- Tr√®s optimis√©
- Support Docker
- Production-ready

### üì¶ Installation avec Docker

```bash
# 1. Installer Docker
curl -fsSL https://get.docker.com | sh

# 2. Lancer TGI avec un mod√®le
docker run --gpus all --shm-size 1g -p 8080:80 \
    -v /data:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Meta-Llama-3.1-8B-Instruct \
    --quantize bitsandbytes

# Sans GPU (CPU uniquement)
docker run --shm-size 1g -p 8080:80 \
    -v /data:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Meta-Llama-3.1-8B-Instruct
```

---

## üîê Acc√®s distant s√©curis√©

### Option A : Reverse Proxy avec Nginx + SSL

```bash
# 1. Installer Nginx et Certbot
sudo apt install nginx certbot python3-certbot-nginx

# 2. Configuration Nginx
sudo nano /etc/nginx/sites-available/summarizer

# Contenu :
server {
    listen 80;
    server_name votre-domaine.com;

    location / {
        proxy_pass http://localhost:8501;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
    }
}

# 3. Activer et obtenir SSL
sudo ln -s /etc/nginx/sites-available/summarizer /etc/nginx/sites-enabled/
sudo certbot --nginx -d votre-domaine.com

# 4. Red√©marrer Nginx
sudo systemctl restart nginx
```

### Option B : Tunnel SSH (Temporaire)

```bash
# Depuis votre PC local
ssh -L 8501:localhost:8501 -L 11434:localhost:11434 user@votre-vps-ip

# Acc√©der localement via :
# http://localhost:8501
```

### Option C : VPN (WireGuard)

Plus s√©curis√© pour un acc√®s permanent. Configuration WireGuard disponible sur demande.

---

## üìä Comparaison des solutions

| Crit√®re | Ollama | vLLM | TGI | Groq (Cloud) |
|---------|--------|------|-----|--------------|
| **Facilit√© d'installation** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Performance CPU** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | N/A |
| **Performance GPU** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | N/A |
| **Gestion m√©moire** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | N/A |
| **Confidentialit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê |
| **Co√ªt** | Gratuit | Gratuit | Gratuit | Limit√© gratuit |
| **Vitesse** | Rapide | Tr√®s rapide | Tr√®s rapide | Ultra rapide |
| **RAM minimum** | 8 GB | 16 GB | 16 GB | 0 |

---

## üéØ Recommandation finale

### Pour vous (VPS sans GPU) :
üëâ **Ollama avec llama3.1:8b-instruct-q4_0**

**Pourquoi ?**
- Installation en 2 minutes
- Fonctionne bien sur CPU
- Consommation RAM optimis√©e (8-12 GB)
- Mod√®les faciles √† changer
- Maintenance minimale

### Commandes compl√®tes pour d√©marrer :

```bash
# Sur votre VPS
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1:8b-instruct-q4_0
sudo systemctl enable ollama
sudo systemctl start ollama

# Tester
curl http://localhost:11434/api/generate -d '{"model":"llama3.1:8b-instruct-q4_0","prompt":"Bonjour!","stream":false}'

# Sur votre PC (cloner le projet)
cd E:\Projets\Summarizer
pip install langchain-community
python -m streamlit run app_local.py

# Dans l'interface, configurer :
# URL Ollama : http://votre-vps-ip:11434
# Mod√®le : llama3.1:8b-instruct-q4_0
```

---

## üìû Support et ressources

- **Documentation Ollama** : https://github.com/ollama/ollama
- **Liste des mod√®les** : https://ollama.com/library
- **Forum Ollama** : https://github.com/ollama/ollama/discussions
- **LangChain + Ollama** : https://python.langchain.com/docs/integrations/llms/ollama

---

## üîÑ Migration Groq ‚Üí Ollama

Pour migrer votre application existante :

1. ‚úÖ J'ai cr√©√© `app_local.py` (version locale)
2. ‚úÖ Lancez Ollama sur votre VPS
3. ‚úÖ Configurez l'URL du serveur dans l'app
4. ‚úÖ Vos donn√©es ne quittent plus votre infrastructure !

**Comparaison :**
- **Avant** : Donn√©es ‚Üí Groq (USA) ‚Üí Retour
- **Maintenant** : Donn√©es ‚Üí Votre VPS (France/EU) ‚Üí Retour

---

Des questions sur l'installation ou besoin d'aide pour configurer votre VPS ? Je suis l√† ! üöÄ
